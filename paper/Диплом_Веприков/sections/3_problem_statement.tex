\section{Постановка задачи} \label{sec:Problem_statement}

Формализуем процесс многократного машинного обучения. В нем каждый шаг соответствует взятию выборки входных данных из вероятностного распределения, обучению модели, оценке предсказаний модели на отложенной тестовой выборке и смешиванию предсказаний с исходными данными для получения нового вероятностного распределения на следующем шаге, что порождает петлю обратной связи \cite{khritankov2021existence}.

Внутреннему состоянию процесса многократного обучения на временном шаге $t$ соответствует случайный вектор $\text{X}_t$. Одной из целей данной работы является описание предельного множества $\lim_{t \to \infty} \text{X}_t$. Пусть $\mathbb{D}_t$ - отображение, такое, что имеет место следующее рекуррентное соотношение

\[
    \text{X}_{t+1} = \mathbb{D}_t (\text{X}_t).
\]

Из предыдущих работ \cite{khritankov2021hidden, khritankov2023positive} можно сделать вывод, что при определенных условиях на $\mathbb{D}_t$ он может быть сжимающим преобразованием и иметь случайную неподвижную точку \citep{itoh1977random}. То есть $\text{X}_{\infty} = \lim\limits_{t \to \infty} \text{X}_{t} = \mathbb{D}_t (\text{X}_{\infty})$. 

Последнее можно интерпретировать следующим образом. Если в пространстве выборок $\Omega \subset \mathbb{R}^n$ задана мера площади $\rho$ и мера вероятности $\text{Pr}$, тогда для любого подмножества $e \subset \Omega$ с мерой $\rho(e)\leq \epsilon$, вероятность $\text{Pr}(\text{X}_t \in e)$ будет стремиться к нулю, если окрестность не содержит неподвижной точки $X_{\infty}$, и к $1$ в противном случае, так что в итоге $\text{Pr}(\text{X}_{\infty} \in e) = 1$. Следовательно, мера области $\rho$ для предельного множества в $\mathbb{R}^n$ будет равна нулю, то есть $\text{X}_{\infty}$ может быть многообразием размерности не более $n-1$, и может существовать ненулевой функционал $L_{\infty}: \mathbb{R}^n \to \mathbb{R}$ такой, что $L_{\infty}(X) = 0$ для любой выборки $X$ из $\text{X}_\infty$.

Пусть $\text{Y}_t$ -- несмещенная оценка $\text{X}_t$, то есть $\mathbb{E}\left[\text{X}_t - \text{Y}_t\right] = 0$, такая, что в пределе $\lim\limits_{t \to \infty} \left(\text{X}_t - \text{Y}_t \right) = 0$.  Для задач обучения с учителем пусть $X_t$ на шаге $t$ является выборкой из $\text{X}_t$ вида $X_t = (A_t, b_t)$. Тогда модель $h_t$ является решением задачи $L_t = L(b_t, h_t(A_{t-1})) \to \min$ с некоторой функцией потерь $L$. В этом случае $Y_t = (A_t, h_t(A_t))$ и $X_t - Y_t = (0, b_t - h_t(A_{t}))$ для любой выборки $X_t$. Поэтому, если существует предел в последовательности невязок $b_t - h_t(A_t)$, то существует и точечный предел разностей $\text{X}_t - \text{Y}_t$. Следовательно, для задач обучения с учителем можно рассматривать только предельное множество случайных векторов вида $\text{b}_t - h_t(\text{A}_t)$.

Как известно, теория отображений случайных векторов хорошо развита только для гладких биективных преобразований для решения случайных дифференциальных уравнений \cite{caraballo2019applying, jornet2021uncertainty}, а это слишком строгое предположение для целей данного исследования. Поэтому вместо того, чтобы подходить к задаче нахождения предельного множества для случайных векторов $\text{X}_t - \text{Y}_t$, мы переформулируем ее с помощью функций плотности невязок модели. 

Рассмотрим множество $\textbf{F}$ функций плотности вероятности и последовательность преобразований $\text{D}_t \in \mathbf{D}$ в пространстве $L_1(\mathbb{R}^n)$, содержащем эти функции. Шаг в процессе многократного обучения соответствует применению отображения $\text{D}_t$ к заданной плотности $f_t \in \textbf{F}$ для получения новой функции плотности $f_{t+1}$.

Таким образом, динамика процесса может быть описана следующим рекуррентным соотношением с номером шага $t$:

\begin{equation}
    \label{system}
    f_{t+1}(x) = \text{D}_t(f_t)(x) ~ \forall x \in \mathbb{R}^n, t \in \mathbb{N}~\text{ и }~ \text{D}_t \in \mathbf{D},
\end{equation}
где $\text{D}_t$ обычно называется оператором эволюции, $f_t(x)$ -- функции плотности вероятности распределения данных, а начальная функция $f_0(x)$ задана. Как уже отмечалось выше, для задачи обучения с учителем система \eqref{system} может быть записана не для функций плотности данных, а для случайных векторов невязок модели.

% Для задачи обучения с учителем система (1) будет иметь вид

По сравнению с рекуррентными соотношениями с линейными операторами или непрерывными динамическими системами с нелинейными операторами эволюции, отображение $\text{D}_t$ в нашем случае является произвольным преобразованием. Мы не предполагаем, что оно должно быть детерминированным, гладким или непрерывным. Например, процесс многократного обучения может использовать стохастический алгоритм обучения, процедуру случайной выборки и другие методы преобразования данных, что делает задачу \eqref{system} важной для изучения.

Если отображения $\{\text{D}_t\}_{t=0}^{\infty} \subset \mathbf{D}$ не зависят от шага $t$, то соотношение \eqref{system} принимает вид

\begin{equation}
    \label{system_aut}
    f_{t+1}(x) = \text{D}(f_t)(x) ~ \forall x \in \mathbb{R}^n \text{ и } t \in \mathbb{N}.
\end{equation}

Разница между этими двумя системами \eqref{system} и \eqref{system_aut} заключается в том, что оператор эволюции в последней остается неизменным для всех $t$, что делает систему автономной, в то время как в первой он может меняться с шагом $t$. Таким образом, алгоритм обучения или его параметры могут меняться со временем в системе \eqref{system}, но оставаться неизменными для системы \eqref{system_aut}.

В работе исследуется предложенная постановка задачи \eqref{system}, чтобы ответить на следующие исследовательские вопросы для процесса многократного обучения.

% Добавить разделы
Во-первых, каковы необходимые и достаточные условия для того, чтобы отображение $D_t$ было преобразованием на множестве функций плотности вероятности $\textbf{F}$. Другими словами, чтобы интересующие нас процессы реального мира и системы искусственного интеллекта можно было представить в виде процесса многократного обучения.

Во-вторых, каким будет предельное поведение системы \eqref{system} с шагом $t$, стремящимся к бесконечности. Важно понять, как повторяющийся характер процесса обучения влияет на работу системы и распределение данных в долгосрочной перспективе.

В-третьих, исследуется, можно ли эмпирически отличить, какая из систем машинного обучения лучше описывается произвольными \eqref{system} или автономными \eqref{system_aut} моделями динамических систем.

В-четвертых, изучено, как изменится предельное множество исходной динамической системы \eqref{system}, при изменении пространства признаков. Значимость данной проблемы следует из стремления обобщить результаты, полученные в предыдущих исследованиях \cite{khritankov2021hidden, khritankov2023positive} и в данной работе, на более сложные модели машинного обучения и разнообразные наборы данных. 

